{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from facebook_scraper import get_posts\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from AttributeRelevance import *\n",
    "from Features import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part - 1: Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to scrape facebook posts\n",
    "def scrape_facebook(accounts):\n",
    "    posts = []\n",
    "    for account in accounts:\n",
    "        for post in get_posts(account):\n",
    "            post_dict = {}\n",
    "            post_dict.update({\"Text\":post['text']})\n",
    "            post_dict.update({\"Likes\":post['likes']})\n",
    "            post_dict.update({\"Comments\":post['comments']})\n",
    "            post_dict.update({\"Shares\":post['shares']})\n",
    "            post_dict.update({\"Account\":account})\n",
    "            # TODO - get subscribers number for a given acccounts\n",
    "            post_dict.update({\"Subscribers\":post['subscribers']})\n",
    "            posts.append(post_dict)\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# facebook pages tobe scrape which arerelated to SRH and GBV\n",
    "fb_page_names = ['SRHMJournal', 'FSRH.UK', 'mmsa.scora', 'BMJ.SRH', \n",
    "                 'BLreproductivehealth', 'actioncanadaSHR', 'GBVPrevNetwork', \n",
    "                 'StopGBVatWork', '16DaysCampaign', 'SayNO.UNiTE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape facebook posts related to SRH and GBV\n",
    "start = time.time()\n",
    "posts = scrape_facebook(fb_page_names)\n",
    "end = time.time()\n",
    "print(f\"data scraping takes: {(end - start)} seconds\")\n",
    "fb_posts_df = pd.DataFrame(posts)\n",
    "print(fb_posts_df.head())\n",
    "print(fb_posts_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - joining the scrapped dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_posts_df.to_csv('../data/fb_posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part - 2: Data Cleaning and Transforming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop amharic posts\n",
    "df = df[df['Text'].apply(lambda txt: not re.search(r\"[\\u1200-\\u137F]+\", str(txt)))]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - content based cleaning \n",
    "# define keyword group count\n",
    "def keyword_group_count(txt):\n",
    "    count = 0\n",
    "    for keyword_list in keywords.values():\n",
    "        for keyword in keyword_list:\n",
    "            if str(keyword).lower() in txt.lower():\n",
    "                count += 1`\n",
    "                break\n",
    "    return count\n",
    "\n",
    "# add keyword group count columns\n",
    "df[\"keyword_group_count\"] = df[\"Text\"].apply(lambda txt: keyword_group_count(txt))\n",
    "df = df[df[\"keyword_group_count\"] >= 5]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label the posts based on the mean of comments, likes, and shares\n",
    "df[[\"Comments\", \"Likes\", \"Shares\"]] = df[[\"Comments\", \"Likes\", \"Shares\"]]/df[\"Subscribers\"]\n",
    "\n",
    "df[\"Comments\"] = df[\"Comments\"].apply(lambda x: 1 if x >= df[\"Comments\"].mean() else 0)\n",
    "df[\"Likes\"] = df[\"Likes\"].apply(lambda x: 1 if x >= df[\"Likes\"].mean() else 0)\n",
    "df[\"Shares\"] = df[\"Shares\"].apply(lambda x: 1 if x >= df[\"Shares\"].mean() else 0)\n",
    "\n",
    "df[\"label\"] = df[[\"Comments\", \"Likes\", \"Shares\"]].mean()\n",
    "df[\"label\"] = df[\"label\"].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add sub-topics columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add content_size column\n",
    "def classifiy_content_size(content_size, quantieles):\n",
    "    if content_size <= quantieles[0]:\n",
    "        return \"short\"\n",
    "    elif content_size >= quantieles[2]:\n",
    "        return \"long\"\n",
    "    else:\n",
    "        return \"meduim\"\n",
    "\n",
    "# extract text content size \n",
    "df[\"content_size\"] = df[\"Text\"].apply(lambda txt: len(txt))\n",
    "(q1, q2, q3, q4) = df[\"content_size\"].quanitile() # TODO - How to find quantile in pandas\n",
    "df[\"content_size\"] = df[\"content_size\"].apply(lambda content_size: classifiy_content_size(content_size, [q1, q2, q3, q4]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add sub_topics columns\n",
    "# TODO - find the relevant sub_topics\n",
    "sub_topics = {\"sub_topics1\": [],\n",
    "              \"sub_topics2\": [],\n",
    "              \"sub_topics3\": []}\n",
    "\n",
    "for (sub_topic_name, sub_topic_list) in sub_topics.items():\n",
    "    df[sub_topic_name] = df['Text'].apply(lambda txt: ','.join([str(sub_topic) for sub_topic in sub_topic_list \\\n",
    "                                                if (sub_topic.lower() in str(txt).lower())]))\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform each element of a list-like to a row, replicating index values.\n",
    "for sub_topic_name in sub_topics.keys():\n",
    "    df['keyword'] = df['keyword'].apply(lambda x: x.split(','))\n",
    "    df = df.explode('keyword')\n",
    "df = df.reset_index().drop(columns='index')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/cleaned_fb_posts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part - 3: Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert sub-topics to bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_dict = {}\n",
    "\n",
    "for col in [c for c in df.columns if (c != 'label' and c != 'Text')]:\n",
    "    feats_dict[col] = CategoricalFeature(df, col)\n",
    "\n",
    "feats = list(feats_dict.values())\n",
    "feats_dict[sub_topic_name].df_lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate woe and iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv = IV()\n",
    "ar = AttributeRelevance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_sum = iv.calculate_iv(feats_dict['keyword'])\n",
    "print(df.head())\n",
    "print(\"iv sum: \", df_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar.analyze(feats, iv, interpretation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize woe and iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub_topic_name in sub_topics.keys():\n",
    "    iv.visualize(feats_dict[sub_topic_name], 'woe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub_topic_name in sub_topics.keys():\n",
    "    iv.visualize(feats_dict[sub_topic_name], 'iv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the woe and iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[sub_topics_name, 'woe', 'iv']]\n",
    "df.to_csv('data/woe_iv_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
